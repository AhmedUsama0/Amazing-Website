{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpyZPrWG6TiBSxJvDNw11e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedUsama0/Amazing-Website/blob/main/Linear%20regression%20model%20for%20multiple%20features\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJdisbKLP2eE",
        "outputId": "c93c398c-20ab-4890-9cdd-efb584208987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66900.0\n",
            "[1.4936e+00 5.6120e+03]\n",
            "iteration    0 with cost 27718626459.658573\n",
            "iteration 1000 with cost 3506097282.389434\n",
            "iteration 2000 with cost 3506097118.691753\n",
            "iteration 3000 with cost 3506097118.691741\n",
            "iteration 4000 with cost 3506097118.69174\n",
            "iteration 5000 with cost 3506097118.69174\n",
            "iteration 6000 with cost 3506097118.69174\n",
            "iteration 7000 with cost 3506097118.69174\n",
            "iteration 8000 with cost 3506097118.69174\n",
            "iteration 9000 with cost 3506097118.69174\n",
            "-2506887.0771388584\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_cost(X, y, w, b):\n",
        "  \"\"\"\n",
        "  Compute the cost value for a linear regression model at specific weights, bias,\n",
        "  and training dataset\n",
        "  Args:\n",
        "  X (ndarray(m,n)): training examples\n",
        "  y (ndarray(m,)) : target values\n",
        "  w (ndarray(n,)) : model weights\n",
        "  b (scalar)      : model bias\n",
        "\n",
        "  Returns:\n",
        "  cost (scalar): the cost value\n",
        "  \"\"\"\n",
        "  m = X.shape[0]\n",
        "  predictions = np.matmul(X,w) + b\n",
        "  error = predictions - y\n",
        "  return (1 / (2*m)) * np.dot(error, error)\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "  \"\"\"\n",
        "  Compute the gradient for model parameters\n",
        "  Args:\n",
        "  X (ndarray(m,n)): training examples\n",
        "  y (ndarray(m,)) : target values\n",
        "  w (ndarray(n,)) : model weights\n",
        "  b (scalar)      : model bias\n",
        "\n",
        "  Returns:\n",
        "  dw (ndarray(n,)): graidents values for weights\n",
        "  db (scalar)     : graident value for bias\n",
        "  \"\"\"\n",
        "  f_wb = np.matmul(X,w) + b\n",
        "  error = f_wb - y\n",
        "  m = X.shape[0]\n",
        "  # X.T is of shape (n,m) and error is of shape (m,) so the result will be of shape (n,)\n",
        "  # Each error in a trainaing example is multiplied by a value of a feature\n",
        "  # rows in X.T represnets values of a feature so summation of multiplication of row by column(error per example) give one gradient value\n",
        "  dj_dw = (1/m) * np.matmul(X.T, error)\n",
        "  dj_db = (1/m) * np.sum(error)\n",
        "\n",
        "  return dj_dw,dj_db\n",
        "\n",
        "def gradient_descent(X, y, w_in, b_in, compute_gradient, compute_cost, alpha, num_iterations):\n",
        "  \"\"\"\n",
        "  Run graident descent algorithm to get the optimal model parameters values\n",
        "  Args:\n",
        "  X (ndarray(m,n))       : training examples\n",
        "  y (ndarray(n,))        : target values\n",
        "  w_in (ndarray(n,))     : initial values of weights\n",
        "  b_in (scalar)          : initial value of bias\n",
        "  compute_gradient       : calculate graident values for parameters\n",
        "  compute_cost           : compute the cost for specific weights and bias values\n",
        "  alpha (scalar)         : learning rate value\n",
        "  num_iterations (scalar): number of iterations to update the parameters\n",
        "\n",
        "  Returns:\n",
        "  w (ndarray(n,)): optimal values for weights\n",
        "  b (scalar): optimal value for bias\n",
        "  \"\"\"\n",
        "  w = w_in.copy()\n",
        "  b = b_in\n",
        "  J_history = []\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    dj_dw,dj_db = compute_gradient(X,y,w,b)\n",
        "\n",
        "    # Update parameters\n",
        "    w = w - alpha*dj_dw\n",
        "    b = b - alpha*dj_db\n",
        "\n",
        "    # Save cost J at each iteration\n",
        "    cost = compute_cost(X, y, w, b)\n",
        "    J_history.append(cost)\n",
        "\n",
        "    if i % math.ceil(num_iterations / 10) == 0:\n",
        "      print(f\"iteration {i:4d} with cost {cost}\")\n",
        "\n",
        "  return w,b,J_history\n",
        "\n",
        "def prediect(x,w,b):\n",
        "  return np.dot(w,x) + b\n",
        "\n",
        "# 1. Load from Colab's built-in sample data\n",
        "df_train = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "# In this file, the target column is called 'median_house_value'\n",
        "X = np.array(df_train[['median_income','total_rooms']])\n",
        "y = np.array(df_train['median_house_value'])\n",
        "print(y[0])\n",
        "print(X[0])\n",
        "\n",
        "# normlaize the data to make the graident descent's job easier and convergence faster\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "X_normalize = (X - mean) / std\n",
        "# Prepare a function to calculate the cost function using MSE\n",
        "w_in = np.array([0,0])\n",
        "b_in = 0\n",
        "iterations = 10000\n",
        "alpha = 0.01\n",
        "\n",
        "w,b,J_history = gradient_descent(X_normalize,y,w_in,b_in,compute_gradient,compute_cost,alpha,iterations)\n",
        "\n",
        "print(prediect(X[0],w,b))\n"
      ]
    }
  ]
}